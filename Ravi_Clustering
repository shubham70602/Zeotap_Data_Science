import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans
from sklearn.metrics import davies_bouldin_score
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns

# Load datasets
customers = pd.read_csv('Customers.csv')
transactions = pd.read_csv('Transactions.csv')

# Merge customers with aggregated transaction data
data = pd.merge(
    customers,
    transactions.groupby('CustomerID').agg(
        total_spent=('TotalValue', 'sum'),
        total_transactions=('TransactionID', 'count'),
        avg_transaction_value=('TotalValue', 'mean'),  # New feature: average transaction value
        purchase_frequency=('TransactionDate', lambda x: len(x.unique()))  # New feature: frequency of purchases
    ).reset_index(),
    on='CustomerID', how='left'
).fillna(0)  # Fill any missing data with 0 (e.g., customers with no transactions)

# Encode categorical 'Region' feature
data['Region'] = LabelEncoder().fit_transform(data['Region'])

# Standardize features for clustering (important for distance-based algorithms like KMeans)
features = StandardScaler().fit_transform(data[['Region', 'total_spent', 'total_transactions', 'avg_transaction_value', 'purchase_frequency']])

# Determine the optimal number of clusters using the Davies-Bouldin index
optimal_k, db_indexes = 2, []
for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, random_state=42).fit(features)
    db_indexes.append(davies_bouldin_score(features, kmeans.labels_))
    if db_indexes[-1] == min(db_indexes):  # Track the k with the minimum DB Index
        optimal_k = k

# Apply KMeans with the optimal number of clusters
data['Cluster'] = KMeans(n_clusters=optimal_k, random_state=42).fit_predict(features)

# Principal Component Analysis (PCA) for visualization of high-dimensional data
pca = PCA(n_components=2)
pca_components = pca.fit_transform(features)

# Enhanced visualization of customer segmentation
plt.figure(figsize=(10, 6))
sns.scatterplot(
    x=pca_components[:, 0], y=pca_components[:, 1], hue=data['Cluster'], palette="coolwarm", s=100, edgecolor="white"
)
plt.title(f"Customer Segmentation with {optimal_k} Clusters", fontsize=16)
plt.xlabel("PCA Component 1", fontsize=12)
plt.ylabel("PCA Component 2", fontsize=12)
plt.legend(title="Cluster", bbox_to_anchor=(1.05, 1), loc="upper left")
plt.grid(True, linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

# Visualize DB Index across different cluster counts
plt.figure(figsize=(8, 5))
plt.plot(range(2, 11), db_indexes, marker='o', color='blue', linestyle='--', linewidth=2)
plt.title("DB Index vs. Number of Clusters", fontsize=16)
plt.xlabel("Number of Clusters", fontsize=12)
plt.ylabel("DB Index", fontsize=12)
plt.grid(True, linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

# Output the results
print(f"Optimal Number of Clusters: {optimal_k}")
print(f"Davies-Bouldin Index for Optimal Clusters: {min(db_indexes):.2f}")
print("Cluster sizes:")
print(data['Cluster'].value_counts())  # Show how many customers are in each cluster
